import joblib
import numpy as np
import pandas as pd
from pydantic import BaseModel, Field
from typing import Optional
import os

# ============== INPUT/OUTPUT SCHEMAS ==============

class PredictionInput(BaseModel):
    """
    RAW INPUT t·ª´ ng∆∞·ªùi d√πng - 8 features c∆° b·∫£n
    Backend s·∫Ω t·ª± ƒë·ªông th·ª±c hi·ªán feature engineering
    """
    rating: float = Field(ge=0.0, le=5.0, description="ƒê√°nh gi√° kh√≥a h·ªçc (0.0 - 5.0)")
    discount: float = Field(ge=0.0, le=1.0, description="M·ª©c gi·∫£m gi√° (0.0 = 0%, 1.0 = 100%)")
    num_reviews: int = Field(ge=0, description="S·ªë l∆∞·ª£ng ƒë√°nh gi√°")
    num_students: int = Field(ge=0, description="S·ªë l∆∞·ª£ng h·ªçc vi√™n")
    price: float = Field(gt=0, description="Gi√° kh√≥a h·ªçc (VND)")
    total_length_minutes: int = Field(gt=0, description="T·ªïng th·ªùi l∆∞·ª£ng (ph√∫t)")
    sections: int = Field(gt=0, description="S·ªë sections")
    lectures: int = Field(gt=0, description="S·ªë lectures")

class PredictionOutput(BaseModel):
    prediction: str
    probability: float

# ============== MODEL CLASS ==============

class UdemyBestsellerModel:
    """
    Model d·ª± ƒëo√°n Udemy Bestseller
    Nh·∫≠n 12 features ƒë√£ engineered t·ª´ frontend, scale v√† predict
    """
    
    def __init__(self):
        # Paths
        model_path = 'model.pkl'
        scaler_path = 'scaler_final.pkl'
        
        # Load model (Random Forest sau GridSearch)
        if os.path.exists(model_path):
            self.model = joblib.load(model_path)
            print(f"‚úì ƒê√£ load model t·ª´ {model_path}")
            print(f"  Model type: {type(self.model).__name__}")
            if hasattr(self.model, 'n_features_in_'):
                print(f"  Number of features: {self.model.n_features_in_}")
        else:
            print(f"‚ö† Warning: {model_path} kh√¥ng t·ªìn t·∫°i.")
            self.model = None
        
        # Load scaler (RobustScaler ƒë√£ fit tr√™n train set)
        if os.path.exists(scaler_path):
            self.scaler = joblib.load(scaler_path)
            print(f"‚úì ƒê√£ load scaler t·ª´ {scaler_path}")
        else:
            print(f"‚ö† Warning: {scaler_path} kh√¥ng t·ªìn t·∫°i.")
            self.scaler = None
        
        # Th·ª© t·ª± features (12 features) - PH·∫¢I ƒê√öNG v·ªõi l√∫c train
        self.FEATURE_NAMES = [
            'rating', 'discount', 'log_num_reviews', 
            'log_num_students', 'log_price', 'log_total_length_minutes', 
            'sqrt_sections', 'sqrt_lectures', 'effective_price', 
            'popularity_score', 'price_per_hour', 'discount_category'
        ]
        
        # Mapping cho classification
        self.target_mapping = {
            0: 'Not Bestseller',
            1: 'Bestseller'
        }
    
    def preprocess_input(self, input_data: PredictionInput) -> pd.DataFrame:
        """
        Ti·ªÅn x·ª≠ l√Ω raw input th√†nh 12 engineered features
        
        C√°c b∆∞·ªõc:
        1. Log transformation: num_reviews, num_students, price, total_length_minutes
        2. Sqrt transformation: sections, lectures
        3. Feature engineering: effective_price, popularity_score, price_per_hour
        4. Discount category encoding
        5. Scaling v·ªõi RobustScaler
        
        Args:
            input_data: PredictionInput v·ªõi 8 raw features
            
        Returns:
            DataFrame ƒë√£ ƒë∆∞·ª£c scale v·ªõi 12 features
        """
        # Chuy·ªÉn input th√†nh dict
        data = input_data.dict()
        df = pd.DataFrame([data])
        
        print("\n" + "="*80)
        print("üì• RAW INPUT (tr∆∞·ªõc khi x·ª≠ l√Ω):")
        print("="*80)
        print(f"  rating: {data['rating']}")
        print(f"  discount: {data['discount']} ({data['discount']*100:.1f}%)")
        print(f"  num_reviews: {data['num_reviews']:,}")
        print(f"  num_students: {data['num_students']:,}")
        print(f"  price: {data['price']:,.0f} VND")
        print(f"  total_length_minutes: {data['total_length_minutes']} min (~{data['total_length_minutes']/60:.1f} hours)")
        print(f"  sections: {data['sections']}")
        print(f"  lectures: {data['lectures']}")
        
        # 1. LOG TRANSFORMATIONS (d√πng log1p ƒë·ªÉ tr√°nh log(0))
        df['log_num_reviews'] = np.log1p(df['num_reviews'])
        df['log_num_students'] = np.log1p(df['num_students'])
        df['log_price'] = np.log1p(df['price'])
        df['log_total_length_minutes'] = np.log1p(df['total_length_minutes'])
        
        # 2. SQRT TRANSFORMATIONS
        df['sqrt_sections'] = np.sqrt(np.clip(df['sections'], 0, None))
        df['sqrt_lectures'] = np.sqrt(np.clip(df['lectures'], 0, None))
        
        # 3. FEATURE ENGINEERING
        # Gi√° hi·ªáu qu·∫£ sau gi·∫£m gi√°
        df['effective_price'] = df['price'] * (1 - np.clip(df['discount'], 0, 1))
        
        # ƒêi·ªÉm ƒë·ªô ph·ªï bi·∫øn (trung b√¨nh students v√† reviews)
        df['popularity_score'] = (df['num_students'] + df['num_reviews']) / 2
        
        # Gi√° m·ªói gi·ªù
        df['price_per_hour'] = df['price'] / (df['total_length_minutes'] / 60 + 1e-3)
        
        # 4. DISCOUNT CATEGORY ENCODING
        # Chia discount th√†nh 3 bins: Low [0-0.3), Medium [0.3-0.6), High [0.6-1.0]
        bins = [0, 0.3, 0.6, 1.0]
        labels = ['Low', 'Medium', 'High']
        
        # pd.cut tr·∫£ v·ªÅ category
        discount_cat = pd.cut(
            np.clip(df['discount'], 0, 1), 
            bins=bins, 
            labels=labels, 
            include_lowest=True
        )
        
        # LabelEncoder m·∫∑c ƒë·ªãnh sort theo alphabet: High=0, Low=1, Medium=2
        le_order = sorted(labels)  # ['High', 'Low', 'Medium']
        discount_map = {name: idx for idx, name in enumerate(le_order)}
        df['discount_category'] = discount_cat.astype(str).map(discount_map).astype(int)
        
        print("\n" + "="*80)
        print("üîß ENGINEERED FEATURES (sau feature engineering):")
        print("="*80)
        print(f"  rating: {df['rating'].values[0]}")
        print(f"  discount: {df['discount'].values[0]}")
        print(f"  log_num_reviews: {df['log_num_reviews'].values[0]:.6f}")
        print(f"  log_num_students: {df['log_num_students'].values[0]:.6f}")
        print(f"  log_price: {df['log_price'].values[0]:.6f}")
        print(f"  log_total_length_minutes: {df['log_total_length_minutes'].values[0]:.6f}")
        print(f"  sqrt_sections: {df['sqrt_sections'].values[0]:.6f}")
        print(f"  sqrt_lectures: {df['sqrt_lectures'].values[0]:.6f}")
        print(f"  effective_price: {df['effective_price'].values[0]:,.2f} VND")
        print(f"  popularity_score: {df['popularity_score'].values[0]:,.2f}")
        print(f"  price_per_hour: {df['price_per_hour'].values[0]:,.2f} VND/hour")
        print(f"  discount_category: {df['discount_category'].values[0]} ({discount_cat.values[0]})")
        
        # 5. CH·ªà GI·ªÆ 12 FEATURES CU·ªêI C√ôNG (ƒë√∫ng th·ª© t·ª±)
        df_model = df[self.FEATURE_NAMES].copy()
        
        print("\n" + "="*80)
        print("üìä FEATURES TR∆Ø·ªöC KHI SCALE:")
        print("="*80)
        print(df_model.to_string(index=False))
        
        # 6. SCALING v·ªõi RobustScaler
        if self.scaler is not None:
            X_scaled = self.scaler.transform(df_model)
            df_scaled = pd.DataFrame(X_scaled, columns=self.FEATURE_NAMES)
            
            print("\n" + "="*80)
            print("‚öñÔ∏è FEATURES SAU KHI SCALE (RobustScaler):")
            print("="*80)
            print(df_scaled.to_string(index=False))
            print("="*80 + "\n")
            
            return df_scaled
        else:
            print("‚ö† Warning: Scaler kh√¥ng t·ªìn t·∫°i, tr·∫£ v·ªÅ data ch∆∞a scale")
            return df_model
    
    def predict(self, input_data: PredictionInput) -> dict:
        """
        D·ª± ƒëo√°n bestseller t·ª´ raw input
        
        Args:
            input_data: PredictionInput v·ªõi 8 raw features
            
        Returns:
            {
                "prediction": "Bestseller" ho·∫∑c "Not Bestseller",
                "probability": float (0-1)
            }
        """
        # Preprocess (feature engineering + scaling)
        X_processed = self.preprocess_input(input_data)
        
        if self.model is not None:
            # D·ª± ƒëo√°n class (0 ho·∫∑c 1)
            prediction_class = self.model.predict(X_processed)[0]
            prediction_label = self.target_mapping[prediction_class]
            
            # L·∫•y probability
            if hasattr(self.model, 'predict_proba'):
                probabilities = self.model.predict_proba(X_processed)[0]
                probability = float(probabilities[prediction_class])
            else:
                probability = 1.0
            
            print(f"‚úì Prediction: {prediction_label} (prob: {probability:.4f})")
            
            return {
                "prediction": prediction_label,
                "probability": probability
            }
        else:
            # Dummy prediction n·∫øu ch∆∞a c√≥ model
            print("‚ö† Warning: Model kh√¥ng t·ªìn t·∫°i, tr·∫£ v·ªÅ dummy prediction")
            return {
                "prediction": "Not Bestseller",
                "probability": 0.5
            }


# ============== KH·ªûI T·∫†O MODEL (Singleton) ==============

# Kh·ªüi t·∫°o model m·ªôt l·∫ßn khi module ƒë∆∞·ª£c import
model = UdemyBestsellerModel()

# Export ƒë·ªÉ d√πng trong FastAPI
__all__ = ['model', 'PredictionInput', 'PredictionOutput']
